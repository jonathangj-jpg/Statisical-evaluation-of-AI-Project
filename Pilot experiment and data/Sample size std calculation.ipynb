{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>score</th>\n",
       "      <th>my</th>\n",
       "      <th>else</th>\n",
       "      <th>nice</th>\n",
       "      <th>honest</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>mistral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     i  score  my  else  nice  honest    model\n",
       "0    0      4   3     3     3       3      gpt\n",
       "1    1      3   3     3     3       3      gpt\n",
       "2    2      4   3     3     3       3      gpt\n",
       "3    3      3   3     3     3       3      gpt\n",
       "4    4      5   3     3     3       3      gpt\n",
       "..  ..    ...  ..   ...   ...     ...      ...\n",
       "95  95      2   3     2     3       3  mistral\n",
       "96  96      2   3     2     3       2  mistral\n",
       "97  97      2   2     2     2       2  mistral\n",
       "98  98      2   2     2     3       2  mistral\n",
       "99  99      2   3     3     3       3  mistral\n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = pd.read_csv(\"gpt-4o-mini_pilotdata.csv\").drop([\"my_sen\", \"else_sen\", \"nice_sen\", \"honest_sen\"], axis=1)\n",
    "mdf = pd.read_csv(\"mistral-small_pilotdata.csv\").drop([\"my_sen\", \"else_sen\", \"nice_sen\", \"honest_sen\"], axis=1)\n",
    "\n",
    "cdf['model'] = 'gpt'\n",
    "mdf['model'] = 'mistral'\n",
    "df_all = pd.concat([cdf, mdf], ignore_index=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt = df_all.melt(id_vars=['i', 'model', 'score'], \n",
    "                      value_vars=['my', 'else', 'nice', 'honest'], \n",
    "                      var_name='prompt', value_name='llm_score')\n",
    "df_alt['abs_dev'] = abs(df_alt['llm_score'] - df_alt['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparison</th>\n",
       "      <th>type</th>\n",
       "      <th>n</th>\n",
       "      <th>std_llm</th>\n",
       "      <th>std_abs_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt: my vs else</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.563539</td>\n",
       "      <td>0.580491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt: my vs nice</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.460566</td>\n",
       "      <td>0.541229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt: my vs honest</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0.510891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt: else vs nice</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.593228</td>\n",
       "      <td>0.751833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt: else vs honest</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.469687</td>\n",
       "      <td>0.469687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt: nice vs honest</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.522233</td>\n",
       "      <td>0.722230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mistral: my vs else</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.544857</td>\n",
       "      <td>0.559491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mistral: my vs nice</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.506922</td>\n",
       "      <td>0.530294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral: my vs honest</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.604361</td>\n",
       "      <td>0.597638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mistral: else vs nice</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.521362</td>\n",
       "      <td>0.702880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mistral: else vs honest</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.481055</td>\n",
       "      <td>0.473436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mistral: nice vs honest</td>\n",
       "      <td>within-model</td>\n",
       "      <td>100</td>\n",
       "      <td>0.518740</td>\n",
       "      <td>0.670896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>my: gpt vs mistral</td>\n",
       "      <td>within-prompt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.626921</td>\n",
       "      <td>0.552222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>else: gpt vs mistral</td>\n",
       "      <td>within-prompt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.555596</td>\n",
       "      <td>0.531816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nice: gpt vs mistral</td>\n",
       "      <td>within-prompt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.490207</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>honest: gpt vs mistral</td>\n",
       "      <td>within-prompt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.519810</td>\n",
       "      <td>0.505625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 comparison           type    n   std_llm  std_abs_dev\n",
       "0           gpt: my vs else   within-model  100  0.563539     0.580491\n",
       "1           gpt: my vs nice   within-model  100  0.460566     0.541229\n",
       "2         gpt: my vs honest   within-model  100  0.471405     0.510891\n",
       "3         gpt: else vs nice   within-model  100  0.593228     0.751833\n",
       "4       gpt: else vs honest   within-model  100  0.469687     0.469687\n",
       "5       gpt: nice vs honest   within-model  100  0.522233     0.722230\n",
       "6       mistral: my vs else   within-model  100  0.544857     0.559491\n",
       "7       mistral: my vs nice   within-model  100  0.506922     0.530294\n",
       "8     mistral: my vs honest   within-model  100  0.604361     0.597638\n",
       "9     mistral: else vs nice   within-model  100  0.521362     0.702880\n",
       "10  mistral: else vs honest   within-model  100  0.481055     0.473436\n",
       "11  mistral: nice vs honest   within-model  100  0.518740     0.670896\n",
       "12       my: gpt vs mistral  within-prompt  100  0.626921     0.552222\n",
       "13     else: gpt vs mistral  within-prompt  100  0.555596     0.531816\n",
       "14     nice: gpt vs mistral  within-prompt  100  0.490207     0.500000\n",
       "15   honest: gpt vs mistral  within-prompt  100  0.519810     0.505625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for model in df_alt['model'].unique():\n",
    "    for p1, p2 in combinations(df_alt['prompt'].unique(), 2):\n",
    "        df1 = df_alt[(df_alt['model'] == model) & (df_alt['prompt'] == p1)].set_index('i')\n",
    "        df2 = df_alt[(df_alt['model'] == model) & (df_alt['prompt'] == p2)].set_index('i')\n",
    "        common = df1.index.intersection(df2.index)\n",
    "        if len(common) == 0:\n",
    "            continue\n",
    "        diff_llm = df1.loc[common]['llm_score'] - df2.loc[common]['llm_score']\n",
    "        diff_abs = df1.loc[common]['abs_dev'] - df2.loc[common]['abs_dev']\n",
    "        results.append({\n",
    "            'comparison': f'{model}: {p1} vs {p2}',\n",
    "            'type': 'within-model',\n",
    "            'n': len(common),\n",
    "            'std_llm': diff_llm.std(),\n",
    "            'std_abs_dev': diff_abs.std()\n",
    "        })\n",
    "\n",
    "for prompt in df_alt['prompt'].unique():\n",
    "    df1 = df_alt[(df_alt['model'] == 'gpt') & (df_alt['prompt'] == prompt)].set_index('i')\n",
    "    df2 = df_alt[(df_alt['model'] == 'mistral') & (df_alt['prompt'] == prompt)].set_index('i')\n",
    "    common = df1.index.intersection(df2.index)\n",
    "    if len(common) == 0:\n",
    "        continue\n",
    "    diff_llm = df1.loc[common]['llm_score'] - df2.loc[common]['llm_score']\n",
    "    diff_abs = df1.loc[common]['abs_dev'] - df2.loc[common]['abs_dev']\n",
    "    results.append({\n",
    "        'comparison': f'{prompt}: gpt vs mistral',\n",
    "        'type': 'within-prompt',\n",
    "        'n': len(common),\n",
    "        'std_llm': diff_llm.std(),\n",
    "        'std_abs_dev': diff_abs.std()\n",
    "    })\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5815665885947171"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum(df_results[\"std_abs_dev\"]**2)/len(df_results[\"std_abs_dev\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303777003706933"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum(df_results[\"std_llm\"]**2)/len(df_results[\"std_llm\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02450",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
